# =========================================================================
#
#  Copyright Ziv Yaniv
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0.txt
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
# =========================================================================

import pandas as pd
import argparse
import sys
import pathlib
from .argparse_types import file_path, dir_path
import requests
import json
from itertools import chain

"""
This script converts the IBEX knowledge-base reagent_resources.csv file to markdown and
adds hyperlinks between the ORCID entries and the supporting material files.
The links cannot exist in the original csv file, or in a simple way in an excel
spreadsheet because we use multiple links in the same table cell (Agree/Disagree), 
functionality excel does not support in a simple manner. # noqa W291

On the other hand, using markdown for the reagent-resources table does not address all our
needs either, sorting and filtering are not possible or are very cumbersome. We 
therefore use the csv file as the official reagent-resources, and it is what
contributors edit.
 
Additionally, the script adds hyperlinks between the entries in the "UniProt Accession Number"
column and their respective entries in the Universal Protein Resource Knowledgebase 
(https://www.uniprot.org/uniprotkb), and between the vendor names and their web-sites based on a user
provided JSON file mapping between the names and URLs. By default the script attempts to get the relevant
web page to confirm existence. If the response is not as expected a warning is printed, but the URL is still
used. We take this approach because some sites are set up to prevent robot scraping and denial of service
attacks making it more complex to check if the URL exists. 
 
The resulting markdown file "reagent_resources.md" is written to the parent directory of the supporting
material.

This script is automatically run when modifications to the reagent_resources.csv file are merged
into the main branch of the ibex_knowledge_base repository (see .github/workflows/data2md.yml).

Assumption: The reagent_resources.csv file is valid. It conforms to the expected format (empty entries denoted 
by the string "NA").
"""

md_header = "<!-- Do NOT edit this file. It is automatically generated from reagents_resources.csv -->\n\n"


def short_circuit_requests_get(url, params=None, **kwargs):
    res = requests.Response()
    # It's always a success
    res.status_code = 200
    return res


request_timeout = 1
request_headers = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"  # noqa E501
}


def json_to_md_str_dict(json_file_path):
    with open(json_file_path) as fp:
        json_dict = json.load(fp)
    md_str_dict = {}
    for raw_text, url_target in json_dict.items():
        try:
            res = requests.get(
                url_target,
                timeout=request_timeout,
                headers=request_headers,
                allow_redirects=True,
            )
            # HTTP 200 status code for success
            if res.status_code == 200:
                md_str_dict[raw_text] = f"[{raw_text}]({url_target})"
            else:
                print(
                    "Warning: problem with {raw_text} URL ({url_target}), check link..."
                )
                md_str_dict[raw_text] = f"[{raw_text}]({url_target})"
        except requests.exceptions.RequestException:
            print(f"Warning: problem with {raw_text} URL ({url_target}), check link...")
            md_str_dict[raw_text] = f"[{raw_text}]({url_target})"
    return md_str_dict


def data_to_md_str(data, supporting_material_root_dir):
    """
    The data parameter is a series with three entries:
    1. String with one or more ORCIDs separated by semicolon, or NA.
    2. "Target Name / Protein Biomarker"
    3. "Conjugate"
    Together these define the path to the supporting material file:
    "Target Name / Protein Biomarker"_"Conjugate"/ORCID.md
    """
    if data[0].strip() == "NA":
        urls_str = "NA"
    else:
        urls_str = ""
        txt = [v.strip() for v in data[0].split(";") if v.strip() != ""]
        for v in txt[0:-1]:
            urls_str += (
                f"[{v}]({supporting_material_root_dir}/{data[1]}_{data[2]}/{v}.md), "
            )
        urls_str += f"[{txt[-1]}]({supporting_material_root_dir}/{data[1]}_{data[2]}/{txt[-1]}.md)"
        # Encode space as %20 in the url
        urls_str = urls_str.replace(" ", "%20")
    return urls_str


def uniprot_to_md_str(uniprot):
    if uniprot == "NA":
        return "NA"
    try:
        # See https://www.uniprot.org/help/api_retrieve_entries
        # We use the rest API url because trying to directly connect
        # to the fixed url always succeeds, return code 200,
        # even when it is an error, the 404 page is shown.
        rest_url_str = f"https://rest.uniprot.org/uniprotkb/{uniprot}.txt"
        res = requests.get(
            rest_url_str,
            timeout=request_timeout,
            headers=request_headers,
            allow_redirects=True,
        )
        # HTTP 200 status code for success
        if res.status_code == 200:
            # Linking to uniprot entries
            return f"[{uniprot}](https://www.uniprot.org/uniprotkb/{uniprot})"
        else:
            return uniprot
    except requests.exceptions.RequestException:
        print(
            f"Warning: problem with {uniprot} URL (https://www.uniprot.org/uniprotkb/{uniprot}), check link..."
        )
        return f"[{uniprot}](https://www.uniprot.org/uniprotkb/{uniprot})"


def uniprots_to_md(uniprots_str, uniprot_md_str):
    uniprots_list = [v.strip() for v in uniprots_str.split(";") if v.strip() != ""]
    urls_str = ""
    for uniprot in uniprots_list[0:-1]:
        urls_str += f"{uniprot_md_str[uniprot]}, "
    urls_str += f"{uniprot_md_str[uniprots_list[-1]]}"
    return urls_str


def csv_to_md_with_url(
    csv_file_path, supporting_material_root_dir, vendor_to_website_json_file_path
):
    """
    Convert the IBEX knowledge-base csv file to markdown and add links to the supporting
    material files. Output is written to a file named markdown.md in the parent directory
    of the supporting_material_root_dir.
    """
    # Read the dataframe and keep entries that are "NA", don't convert to nan
    df = pd.read_csv(csv_file_path, dtype=str, keep_default_na=False)
    supporting_material_path = pathlib.PurePath(supporting_material_root_dir).name
    if not df.empty:
        print("Start linking to supporting material...")
        df["Agree"] = df[
            ["Agree", "Target Name / Protein Biomarker", "Conjugate"]
        ].apply(lambda x: data_to_md_str(x, supporting_material_path), axis=1)
        df["Disagree"] = df[
            ["Disagree", "Target Name / Protein Biomarker", "Conjugate"]
        ].apply(
            lambda x: data_to_md_str(
                x, pathlib.PurePath(supporting_material_root_dir).name
            ),
            axis=1,
        )
        print("Finished linking to supporting material...")
        print("Start linking to UniProt...")
        # Link to the UniProt Knowledgebase. Get the unique uniprots and the corresponding
        # hyperlinked markdown string.
        unique_uniports = set(
            chain(
                *df["UniProt Accession Number"].apply(
                    lambda x: [v.strip() for v in x.split(";")]
                )
            )
        )
        uniprot_md_str = {}
        for uniprot in unique_uniports:
            uniprot_md_str[uniprot] = uniprot_to_md_str(uniprot)
        df["UniProt Accession Number"] = df["UniProt Accession Number"].apply(
            lambda x: uniprots_to_md(x, uniprot_md_str)
        )
        print("Finished linking to UniProt...")
        print("Start linking to vendor websites...")
        vendor_to_website = json_to_md_str_dict(vendor_to_website_json_file_path)
        df["Vendor"] = df["Vendor"].apply(lambda x: vendor_to_website[x])
        print("Finished linking to vendor websites...")
    with open(supporting_material_root_dir.parent / "reagent_resources.md", "w") as fp:
        fp.write("# Reagent Resources\n\n" + md_header + df.to_markdown(index=False))


def main(argv=None):
    if argv is None:  # script was invoked from commandline
        argv = sys.argv[1:]
    parser = argparse.ArgumentParser(
        description="Convert knowledge-base reagent resources file from csv to md and add hyperlinks."
    )
    parser.add_argument(
        "csv_file", type=file_path, help="Path to the reagent_resources.csv file."
    )
    parser.add_argument(
        "supporting_material_root_dir",
        type=dir_path,
        help="Path to the directory containing the supporting materials files.",
    )
    parser.add_argument(
        "vendor_to_website",
        type=file_path,
        help="JSON file containing the mapping between vendor name and website",
    )
    parser.add_argument(
        "--skip_url_validation",
        action="store_true",
        help="Skip validation of vendor and UniProt urls.",
    )
    args = parser.parse_args(argv)

    try:
        if args.skip_url_validation:
            requests.get = short_circuit_requests_get
        csv_to_md_with_url(
            args.csv_file, args.supporting_material_root_dir, args.vendor_to_website
        )
    except Exception as e:
        print(
            f"{e}",
            file=sys.stderr,
        )
        return 1
    return 0


if __name__ == "__main__":
    sys.exit(main())
